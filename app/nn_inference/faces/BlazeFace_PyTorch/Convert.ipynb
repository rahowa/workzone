{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert TFLite model to PyTorch\n",
    "\n",
    "This uses the model **face_detection_front.tflite** from [MediaPipe](https://github.com/google/mediapipe/tree/master/mediapipe/models).\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "1) Clone the MediaPipe repo:\n",
    "\n",
    "```\n",
    "git clone https://github.com/google/mediapipe.git\n",
    "```\n",
    "\n",
    "2) Install **flatbuffers**:\n",
    "\n",
    "```\n",
    "git clone https://github.com/google/flatbuffers.git\n",
    "cmake -G \"Unix Makefiles\" -DCMAKE_BUILD_TYPE=Release\n",
    "make -j\n",
    "\n",
    "cd flatbuffers/python\n",
    "python setup.py install\n",
    "```\n",
    "\n",
    "3) Clone the TensorFlow repo. We only need this to get the FlatBuffers schema files (I guess you could just download [schema.fbs](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs)).\n",
    "\n",
    "```\n",
    "git clone https://github.com/tensorflow/tensorflow.git\n",
    "```\n",
    "\n",
    "4) Convert the schema files to Python files using **flatc**:\n",
    "\n",
    "```\n",
    "./flatbuffers/flatc --python tensorflow/tensorflow/lite/schema/schema.fbs\n",
    "```\n",
    "\n",
    "Now we can use the Python FlatBuffer API to read the TFLite file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the weights from the TFLite file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the TFLite model using the FlatBuffers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflite import Model\n",
    "\n",
    "data = open(\"./mediapipe/mediapipe/models/face_detection_front.tflite\", \"rb\").read()\n",
    "model = Model.Model.GetRootAsModel(data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'facedetector-front.tflite.no_meta'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgraph = model.Subgraphs(0)\n",
    "subgraph.Name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(tensor):\n",
    "    return [tensor.Shape(i) for i in range(tensor.ShapeLength())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all the tensors in the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0                       b'input' 0  0 [1, 128, 128, 3]\n",
      "  1               b'conv2d/Kernel' 0  1 [24, 5, 5, 3]\n",
      "  2                 b'conv2d/Bias' 0  2 [24]\n",
      "  3                      b'conv2d' 0  0 [1, 64, 64, 24]\n",
      "  4                  b'activation' 0  0 [1, 64, 64, 24]\n",
      "  5     b'depthwise_conv2d/Kernel' 0  3 [1, 3, 3, 24]\n",
      "  6       b'depthwise_conv2d/Bias' 0  4 [24]\n",
      "  7            b'depthwise_conv2d' 0  0 [1, 64, 64, 24]\n",
      "  8             b'conv2d_1/Kernel' 0  5 [24, 1, 1, 24]\n",
      "  9               b'conv2d_1/Bias' 0  6 [24]\n",
      " 10                    b'conv2d_1' 0  0 [1, 64, 64, 24]\n",
      " 11                         b'add' 0  0 [1, 64, 64, 24]\n",
      " 12                b'activation_1' 0  0 [1, 64, 64, 24]\n",
      " 13   b'depthwise_conv2d_1/Kernel' 0  7 [1, 3, 3, 24]\n",
      " 14     b'depthwise_conv2d_1/Bias' 0  8 [24]\n",
      " 15          b'depthwise_conv2d_1' 0  0 [1, 64, 64, 24]\n",
      " 16             b'conv2d_2/Kernel' 0  9 [28, 1, 1, 24]\n",
      " 17               b'conv2d_2/Bias' 0 10 [28]\n",
      " 18                    b'conv2d_2' 0  0 [1, 64, 64, 28]\n",
      " 19    b'channel_padding/Paddings' 2 11 [4, 2]\n",
      " 20             b'channel_padding' 0  0 [1, 64, 64, 28]\n",
      " 21                       b'add_1' 0  0 [1, 64, 64, 28]\n",
      " 22                b'activation_2' 0  0 [1, 64, 64, 28]\n",
      " 23   b'depthwise_conv2d_2/Kernel' 0 12 [1, 3, 3, 28]\n",
      " 24     b'depthwise_conv2d_2/Bias' 0 13 [28]\n",
      " 25          b'depthwise_conv2d_2' 0  0 [1, 32, 32, 28]\n",
      " 26               b'max_pooling2d' 0  0 [1, 32, 32, 28]\n",
      " 27             b'conv2d_3/Kernel' 0 14 [32, 1, 1, 28]\n",
      " 28               b'conv2d_3/Bias' 0 15 [32]\n",
      " 29                    b'conv2d_3' 0  0 [1, 32, 32, 32]\n",
      " 30  b'channel_padding_1/Paddings' 2 16 [4, 2]\n",
      " 31           b'channel_padding_1' 0  0 [1, 32, 32, 32]\n",
      " 32                       b'add_2' 0  0 [1, 32, 32, 32]\n",
      " 33                b'activation_3' 0  0 [1, 32, 32, 32]\n",
      " 34   b'depthwise_conv2d_3/Kernel' 0 17 [1, 3, 3, 32]\n",
      " 35     b'depthwise_conv2d_3/Bias' 0 18 [32]\n",
      " 36          b'depthwise_conv2d_3' 0  0 [1, 32, 32, 32]\n",
      " 37             b'conv2d_4/Kernel' 0 19 [36, 1, 1, 32]\n",
      " 38               b'conv2d_4/Bias' 0 20 [36]\n",
      " 39                    b'conv2d_4' 0  0 [1, 32, 32, 36]\n",
      " 40  b'channel_padding_2/Paddings' 2 21 [4, 2]\n",
      " 41           b'channel_padding_2' 0  0 [1, 32, 32, 36]\n",
      " 42                       b'add_3' 0  0 [1, 32, 32, 36]\n",
      " 43                b'activation_4' 0  0 [1, 32, 32, 36]\n",
      " 44   b'depthwise_conv2d_4/Kernel' 0 22 [1, 3, 3, 36]\n",
      " 45     b'depthwise_conv2d_4/Bias' 0 23 [36]\n",
      " 46          b'depthwise_conv2d_4' 0  0 [1, 32, 32, 36]\n",
      " 47             b'conv2d_5/Kernel' 0 24 [42, 1, 1, 36]\n",
      " 48               b'conv2d_5/Bias' 0 25 [42]\n",
      " 49                    b'conv2d_5' 0  0 [1, 32, 32, 42]\n",
      " 50  b'channel_padding_3/Paddings' 2 26 [4, 2]\n",
      " 51           b'channel_padding_3' 0  0 [1, 32, 32, 42]\n",
      " 52                       b'add_4' 0  0 [1, 32, 32, 42]\n",
      " 53                b'activation_5' 0  0 [1, 32, 32, 42]\n",
      " 54   b'depthwise_conv2d_5/Kernel' 0 27 [1, 3, 3, 42]\n",
      " 55     b'depthwise_conv2d_5/Bias' 0 28 [42]\n",
      " 56          b'depthwise_conv2d_5' 0  0 [1, 16, 16, 42]\n",
      " 57             b'max_pooling2d_1' 0  0 [1, 16, 16, 42]\n",
      " 58             b'conv2d_6/Kernel' 0 29 [48, 1, 1, 42]\n",
      " 59               b'conv2d_6/Bias' 0 30 [48]\n",
      " 60                    b'conv2d_6' 0  0 [1, 16, 16, 48]\n",
      " 61  b'channel_padding_4/Paddings' 2 31 [4, 2]\n",
      " 62           b'channel_padding_4' 0  0 [1, 16, 16, 48]\n",
      " 63                       b'add_5' 0  0 [1, 16, 16, 48]\n",
      " 64                b'activation_6' 0  0 [1, 16, 16, 48]\n",
      " 65   b'depthwise_conv2d_6/Kernel' 0 32 [1, 3, 3, 48]\n",
      " 66     b'depthwise_conv2d_6/Bias' 0 33 [48]\n",
      " 67          b'depthwise_conv2d_6' 0  0 [1, 16, 16, 48]\n",
      " 68             b'conv2d_7/Kernel' 0 34 [56, 1, 1, 48]\n",
      " 69               b'conv2d_7/Bias' 0 35 [56]\n",
      " 70                    b'conv2d_7' 0  0 [1, 16, 16, 56]\n",
      " 71  b'channel_padding_5/Paddings' 2 36 [4, 2]\n",
      " 72           b'channel_padding_5' 0  0 [1, 16, 16, 56]\n",
      " 73                       b'add_6' 0  0 [1, 16, 16, 56]\n",
      " 74                b'activation_7' 0  0 [1, 16, 16, 56]\n",
      " 75   b'depthwise_conv2d_7/Kernel' 0 37 [1, 3, 3, 56]\n",
      " 76     b'depthwise_conv2d_7/Bias' 0 38 [56]\n",
      " 77          b'depthwise_conv2d_7' 0  0 [1, 16, 16, 56]\n",
      " 78             b'conv2d_8/Kernel' 0 39 [64, 1, 1, 56]\n",
      " 79               b'conv2d_8/Bias' 0 40 [64]\n",
      " 80                    b'conv2d_8' 0  0 [1, 16, 16, 64]\n",
      " 81  b'channel_padding_6/Paddings' 2 41 [4, 2]\n",
      " 82           b'channel_padding_6' 0  0 [1, 16, 16, 64]\n",
      " 83                       b'add_7' 0  0 [1, 16, 16, 64]\n",
      " 84                b'activation_8' 0  0 [1, 16, 16, 64]\n",
      " 85   b'depthwise_conv2d_8/Kernel' 0 42 [1, 3, 3, 64]\n",
      " 86     b'depthwise_conv2d_8/Bias' 0 43 [64]\n",
      " 87          b'depthwise_conv2d_8' 0  0 [1, 16, 16, 64]\n",
      " 88             b'conv2d_9/Kernel' 0 44 [72, 1, 1, 64]\n",
      " 89               b'conv2d_9/Bias' 0 45 [72]\n",
      " 90                    b'conv2d_9' 0  0 [1, 16, 16, 72]\n",
      " 91  b'channel_padding_7/Paddings' 2 46 [4, 2]\n",
      " 92           b'channel_padding_7' 0  0 [1, 16, 16, 72]\n",
      " 93                       b'add_8' 0  0 [1, 16, 16, 72]\n",
      " 94                b'activation_9' 0  0 [1, 16, 16, 72]\n",
      " 95   b'depthwise_conv2d_9/Kernel' 0 47 [1, 3, 3, 72]\n",
      " 96     b'depthwise_conv2d_9/Bias' 0 48 [72]\n",
      " 97          b'depthwise_conv2d_9' 0  0 [1, 16, 16, 72]\n",
      " 98            b'conv2d_10/Kernel' 0 49 [80, 1, 1, 72]\n",
      " 99              b'conv2d_10/Bias' 0 50 [80]\n",
      "100                   b'conv2d_10' 0  0 [1, 16, 16, 80]\n",
      "101  b'channel_padding_8/Paddings' 2 51 [4, 2]\n",
      "102           b'channel_padding_8' 0  0 [1, 16, 16, 80]\n",
      "103                       b'add_9' 0  0 [1, 16, 16, 80]\n",
      "104               b'activation_10' 0  0 [1, 16, 16, 80]\n",
      "105  b'depthwise_conv2d_10/Kernel' 0 52 [1, 3, 3, 80]\n",
      "106    b'depthwise_conv2d_10/Bias' 0 53 [80]\n",
      "107         b'depthwise_conv2d_10' 0  0 [1, 16, 16, 80]\n",
      "108            b'conv2d_11/Kernel' 0 54 [88, 1, 1, 80]\n",
      "109              b'conv2d_11/Bias' 0 55 [88]\n",
      "110                   b'conv2d_11' 0  0 [1, 16, 16, 88]\n",
      "111  b'channel_padding_9/Paddings' 2 56 [4, 2]\n",
      "112           b'channel_padding_9' 0  0 [1, 16, 16, 88]\n",
      "113                      b'add_10' 0  0 [1, 16, 16, 88]\n",
      "114               b'activation_11' 0  0 [1, 16, 16, 88]\n",
      "115  b'depthwise_conv2d_11/Kernel' 0 57 [1, 3, 3, 88]\n",
      "116    b'depthwise_conv2d_11/Bias' 0 58 [88]\n",
      "117         b'depthwise_conv2d_11' 0  0 [1, 8, 8, 88]\n",
      "118             b'max_pooling2d_2' 0  0 [1, 8, 8, 88]\n",
      "119            b'conv2d_12/Kernel' 0 59 [96, 1, 1, 88]\n",
      "120              b'conv2d_12/Bias' 0 60 [96]\n",
      "121                   b'conv2d_12' 0  0 [1, 8, 8, 96]\n",
      "122 b'channel_padding_10/Paddings' 2 61 [4, 2]\n",
      "123          b'channel_padding_10' 0  0 [1, 8, 8, 96]\n",
      "124                      b'add_11' 0  0 [1, 8, 8, 96]\n",
      "125               b'activation_12' 0  0 [1, 8, 8, 96]\n",
      "126  b'depthwise_conv2d_12/Kernel' 0 62 [1, 3, 3, 96]\n",
      "127    b'depthwise_conv2d_12/Bias' 0 63 [96]\n",
      "128         b'depthwise_conv2d_12' 0  0 [1, 8, 8, 96]\n",
      "129            b'conv2d_13/Kernel' 0 64 [96, 1, 1, 96]\n",
      "130              b'conv2d_13/Bias' 0 65 [96]\n",
      "131                   b'conv2d_13' 0  0 [1, 8, 8, 96]\n",
      "132                      b'add_12' 0  0 [1, 8, 8, 96]\n",
      "133               b'activation_13' 0  0 [1, 8, 8, 96]\n",
      "134  b'depthwise_conv2d_13/Kernel' 0 66 [1, 3, 3, 96]\n",
      "135    b'depthwise_conv2d_13/Bias' 0 67 [96]\n",
      "136         b'depthwise_conv2d_13' 0  0 [1, 8, 8, 96]\n",
      "137            b'conv2d_14/Kernel' 0 68 [96, 1, 1, 96]\n",
      "138              b'conv2d_14/Bias' 0 69 [96]\n",
      "139                   b'conv2d_14' 0  0 [1, 8, 8, 96]\n",
      "140                      b'add_13' 0  0 [1, 8, 8, 96]\n",
      "141               b'activation_14' 0  0 [1, 8, 8, 96]\n",
      "142  b'depthwise_conv2d_14/Kernel' 0 70 [1, 3, 3, 96]\n",
      "143    b'depthwise_conv2d_14/Bias' 0 71 [96]\n",
      "144         b'depthwise_conv2d_14' 0  0 [1, 8, 8, 96]\n",
      "145            b'conv2d_15/Kernel' 0 72 [96, 1, 1, 96]\n",
      "146              b'conv2d_15/Bias' 0 73 [96]\n",
      "147                   b'conv2d_15' 0  0 [1, 8, 8, 96]\n",
      "148                      b'add_14' 0  0 [1, 8, 8, 96]\n",
      "149               b'activation_15' 0  0 [1, 8, 8, 96]\n",
      "150  b'depthwise_conv2d_15/Kernel' 0 74 [1, 3, 3, 96]\n",
      "151    b'depthwise_conv2d_15/Bias' 0 75 [96]\n",
      "152         b'depthwise_conv2d_15' 0  0 [1, 8, 8, 96]\n",
      "153            b'conv2d_16/Kernel' 0 76 [96, 1, 1, 96]\n",
      "154              b'conv2d_16/Bias' 0 77 [96]\n",
      "155                   b'conv2d_16' 0  0 [1, 8, 8, 96]\n",
      "156                      b'add_15' 0  0 [1, 8, 8, 96]\n",
      "157               b'activation_16' 0  0 [1, 8, 8, 96]\n",
      "158      b'classificator_8/Kernel' 0 78 [2, 1, 1, 88]\n",
      "159        b'classificator_8/Bias' 0 79 [2]\n",
      "160             b'classificator_8' 0  0 [1, 16, 16, 2]\n",
      "161     b'classificator_16/Kernel' 0 80 [6, 1, 1, 96]\n",
      "162       b'classificator_16/Bias' 0 81 [6]\n",
      "163            b'classificator_16' 0  0 [1, 8, 8, 6]\n",
      "164          b'regressor_8/Kernel' 0 82 [32, 1, 1, 88]\n",
      "165            b'regressor_8/Bias' 0 83 [32]\n",
      "166                 b'regressor_8' 0  0 [1, 16, 16, 32]\n",
      "167         b'regressor_16/Kernel' 0 84 [96, 1, 1, 96]\n",
      "168           b'regressor_16/Bias' 0 85 [96]\n",
      "169                b'regressor_16' 0  0 [1, 8, 8, 96]\n",
      "170                     b'reshape' 0  0 [1, 512, 1]\n",
      "171                   b'reshape_2' 0  0 [1, 384, 1]\n",
      "172                   b'reshape_1' 0  0 [1, 512, 16]\n",
      "173                   b'reshape_3' 0  0 [1, 384, 16]\n",
      "174              b'classificators' 0  0 [1, 896, 1]\n",
      "175                  b'regressors' 0  0 [1, 896, 16]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, subgraph.TensorsLength()):\n",
    "    tensor = subgraph.Tensors(i)\n",
    "    print(\"%3d %30s %d %2d %s\" % (i, tensor.Name(), tensor.Type(), tensor.Buffer(), \n",
    "                                  get_shape(subgraph.Tensors(i))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a look-up table that lets us get the tensor index based on the tensor name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dict = {(subgraph.Tensors(i).Name().decode(\"utf8\")): i \n",
    "               for i in range(subgraph.TensorsLength())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab only the tensors that represent weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {}\n",
    "for i in range(subgraph.TensorsLength()):\n",
    "    tensor = subgraph.Tensors(i)\n",
    "    if tensor.Buffer() > 0:\n",
    "        name = tensor.Name().decode(\"utf8\")\n",
    "        parameters[name] = tensor.Buffer()\n",
    "\n",
    "len(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The buffers are simply arrays of bytes. As the docs say,\n",
    "\n",
    "> The data_buffer itself is an opaque container, with the assumption that the\n",
    "> target device is little-endian. In addition, all builtin operators assume\n",
    "> the memory is ordered such that if `shape` is [4, 3, 2], then index\n",
    "> [i, j, k] maps to `data_buffer[i*3*2 + j*2 + k]`.\n",
    "\n",
    "For weights and biases, we need to interpret every 4 bytes as being as float. On my machine, the native byte ordering is already little-endian so we don't need to do anything special for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(tensor_name):\n",
    "    i = tensor_dict[tensor_name]\n",
    "    tensor = subgraph.Tensors(i)\n",
    "    buffer = tensor.Buffer()\n",
    "    shape = get_shape(tensor)\n",
    "    assert(tensor.Type() == 0)  # FLOAT32\n",
    "    \n",
    "    W = model.Buffers(buffer).DataAsNumpy()\n",
    "    W = W.view(dtype=np.float32)\n",
    "    W = W.reshape(shape)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24, 5, 5, 3), (24,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = get_weights(\"conv2d/Kernel\")\n",
    "b = get_weights(\"conv2d/Bias\")\n",
    "W.shape, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the weights for all the layers and copy them into our PyTorch model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the weights to PyTorch format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from blazeface import BlazeFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BlazeFace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlazeFace(\n",
       "  (backbone1): Sequential(\n",
       "    (0): Conv2d(3, 24, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)\n",
       "        (1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace)\n",
       "    )\n",
       "    (3): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)\n",
       "        (1): Conv2d(24, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace)\n",
       "    )\n",
       "    (4): BlazeBlock(\n",
       "      (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(28, 28, kernel_size=(3, 3), stride=(2, 2), groups=28)\n",
       "        (1): Conv2d(28, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace)\n",
       "    )\n",
       "    (5): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "        (1): Conv2d(32, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace)\n",
       "    )\n",
       "    (6): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36)\n",
       "        (1): Conv2d(36, 42, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace)\n",
       "    )\n",
       "    (7): BlazeBlock(\n",
       "      (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(42, 42, kernel_size=(3, 3), stride=(2, 2), groups=42)\n",
       "        (1): Conv2d(42, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace)\n",
       "    )\n",
       "    (8): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
       "        (1): Conv2d(48, 56, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace)\n",
       "    )\n",
       "    (9): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=56)\n",
       "        (1): Conv2d(56, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace)\n",
       "    )\n",
       "    (10): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "        (1): Conv2d(64, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace)\n",
       "    )\n",
       "    (11): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)\n",
       "        (1): Conv2d(72, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace)\n",
       "    )\n",
       "    (12): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80)\n",
       "        (1): Conv2d(80, 88, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (backbone2): Sequential(\n",
       "    (0): BlazeBlock(\n",
       "      (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(2, 2), groups=88)\n",
       "        (1): Conv2d(88, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace)\n",
       "    )\n",
       "    (1): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "        (1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace)\n",
       "    )\n",
       "    (2): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "        (1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace)\n",
       "    )\n",
       "    (3): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "        (1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace)\n",
       "    )\n",
       "    (4): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "        (1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (classifier_8): Conv2d(88, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (classifier_16): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (regressor_8): Conv2d(88, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (regressor_16): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a lookup table that maps the layer names between the two models. We're going to assume here that the tensors will be in the same order in both models. If not, we should get an error because shapes don't match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conv2d/Kernel',\n",
       " 'conv2d/Bias',\n",
       " 'depthwise_conv2d/Kernel',\n",
       " 'depthwise_conv2d/Bias',\n",
       " 'conv2d_1/Kernel']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probable_names = []\n",
    "for i in range(0, subgraph.TensorsLength()):\n",
    "    tensor = subgraph.Tensors(i)\n",
    "    if tensor.Buffer() > 0 and tensor.Type() == 0:\n",
    "        probable_names.append(tensor.Name().decode(\"utf-8\"))\n",
    "        \n",
    "probable_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert = {}\n",
    "i = 0\n",
    "for name, params in net.state_dict().items():\n",
    "    convert[name] = probable_names[i]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the weights into the layers.\n",
    "\n",
    "Note that the ordering of the weights is different between PyTorch and TFLite, so we need to transpose them.\n",
    "\n",
    "Convolution weights:\n",
    "\n",
    "    TFLite:  (out_channels, kernel_height, kernel_width, in_channels)\n",
    "    PyTorch: (out_channels, in_channels, kernel_height, kernel_width)\n",
    "\n",
    "Depthwise convolution weights:\n",
    "\n",
    "    TFLite:  (1, kernel_height, kernel_width, channels)\n",
    "    PyTorch: (channels, 1, kernel_height, kernel_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone1.0.weight conv2d/Kernel (24, 5, 5, 3) torch.Size([24, 3, 5, 5])\n",
      "backbone1.0.bias conv2d/Bias (24,) torch.Size([24])\n",
      "backbone1.2.convs.0.weight depthwise_conv2d/Kernel (1, 3, 3, 24) torch.Size([24, 1, 3, 3])\n",
      "backbone1.2.convs.0.bias depthwise_conv2d/Bias (24,) torch.Size([24])\n",
      "backbone1.2.convs.1.weight conv2d_1/Kernel (24, 1, 1, 24) torch.Size([24, 24, 1, 1])\n",
      "backbone1.2.convs.1.bias conv2d_1/Bias (24,) torch.Size([24])\n",
      "backbone1.3.convs.0.weight depthwise_conv2d_1/Kernel (1, 3, 3, 24) torch.Size([24, 1, 3, 3])\n",
      "backbone1.3.convs.0.bias depthwise_conv2d_1/Bias (24,) torch.Size([24])\n",
      "backbone1.3.convs.1.weight conv2d_2/Kernel (28, 1, 1, 24) torch.Size([28, 24, 1, 1])\n",
      "backbone1.3.convs.1.bias conv2d_2/Bias (28,) torch.Size([28])\n",
      "backbone1.4.convs.0.weight depthwise_conv2d_2/Kernel (1, 3, 3, 28) torch.Size([28, 1, 3, 3])\n",
      "backbone1.4.convs.0.bias depthwise_conv2d_2/Bias (28,) torch.Size([28])\n",
      "backbone1.4.convs.1.weight conv2d_3/Kernel (32, 1, 1, 28) torch.Size([32, 28, 1, 1])\n",
      "backbone1.4.convs.1.bias conv2d_3/Bias (32,) torch.Size([32])\n",
      "backbone1.5.convs.0.weight depthwise_conv2d_3/Kernel (1, 3, 3, 32) torch.Size([32, 1, 3, 3])\n",
      "backbone1.5.convs.0.bias depthwise_conv2d_3/Bias (32,) torch.Size([32])\n",
      "backbone1.5.convs.1.weight conv2d_4/Kernel (36, 1, 1, 32) torch.Size([36, 32, 1, 1])\n",
      "backbone1.5.convs.1.bias conv2d_4/Bias (36,) torch.Size([36])\n",
      "backbone1.6.convs.0.weight depthwise_conv2d_4/Kernel (1, 3, 3, 36) torch.Size([36, 1, 3, 3])\n",
      "backbone1.6.convs.0.bias depthwise_conv2d_4/Bias (36,) torch.Size([36])\n",
      "backbone1.6.convs.1.weight conv2d_5/Kernel (42, 1, 1, 36) torch.Size([42, 36, 1, 1])\n",
      "backbone1.6.convs.1.bias conv2d_5/Bias (42,) torch.Size([42])\n",
      "backbone1.7.convs.0.weight depthwise_conv2d_5/Kernel (1, 3, 3, 42) torch.Size([42, 1, 3, 3])\n",
      "backbone1.7.convs.0.bias depthwise_conv2d_5/Bias (42,) torch.Size([42])\n",
      "backbone1.7.convs.1.weight conv2d_6/Kernel (48, 1, 1, 42) torch.Size([48, 42, 1, 1])\n",
      "backbone1.7.convs.1.bias conv2d_6/Bias (48,) torch.Size([48])\n",
      "backbone1.8.convs.0.weight depthwise_conv2d_6/Kernel (1, 3, 3, 48) torch.Size([48, 1, 3, 3])\n",
      "backbone1.8.convs.0.bias depthwise_conv2d_6/Bias (48,) torch.Size([48])\n",
      "backbone1.8.convs.1.weight conv2d_7/Kernel (56, 1, 1, 48) torch.Size([56, 48, 1, 1])\n",
      "backbone1.8.convs.1.bias conv2d_7/Bias (56,) torch.Size([56])\n",
      "backbone1.9.convs.0.weight depthwise_conv2d_7/Kernel (1, 3, 3, 56) torch.Size([56, 1, 3, 3])\n",
      "backbone1.9.convs.0.bias depthwise_conv2d_7/Bias (56,) torch.Size([56])\n",
      "backbone1.9.convs.1.weight conv2d_8/Kernel (64, 1, 1, 56) torch.Size([64, 56, 1, 1])\n",
      "backbone1.9.convs.1.bias conv2d_8/Bias (64,) torch.Size([64])\n",
      "backbone1.10.convs.0.weight depthwise_conv2d_8/Kernel (1, 3, 3, 64) torch.Size([64, 1, 3, 3])\n",
      "backbone1.10.convs.0.bias depthwise_conv2d_8/Bias (64,) torch.Size([64])\n",
      "backbone1.10.convs.1.weight conv2d_9/Kernel (72, 1, 1, 64) torch.Size([72, 64, 1, 1])\n",
      "backbone1.10.convs.1.bias conv2d_9/Bias (72,) torch.Size([72])\n",
      "backbone1.11.convs.0.weight depthwise_conv2d_9/Kernel (1, 3, 3, 72) torch.Size([72, 1, 3, 3])\n",
      "backbone1.11.convs.0.bias depthwise_conv2d_9/Bias (72,) torch.Size([72])\n",
      "backbone1.11.convs.1.weight conv2d_10/Kernel (80, 1, 1, 72) torch.Size([80, 72, 1, 1])\n",
      "backbone1.11.convs.1.bias conv2d_10/Bias (80,) torch.Size([80])\n",
      "backbone1.12.convs.0.weight depthwise_conv2d_10/Kernel (1, 3, 3, 80) torch.Size([80, 1, 3, 3])\n",
      "backbone1.12.convs.0.bias depthwise_conv2d_10/Bias (80,) torch.Size([80])\n",
      "backbone1.12.convs.1.weight conv2d_11/Kernel (88, 1, 1, 80) torch.Size([88, 80, 1, 1])\n",
      "backbone1.12.convs.1.bias conv2d_11/Bias (88,) torch.Size([88])\n",
      "backbone2.0.convs.0.weight depthwise_conv2d_11/Kernel (1, 3, 3, 88) torch.Size([88, 1, 3, 3])\n",
      "backbone2.0.convs.0.bias depthwise_conv2d_11/Bias (88,) torch.Size([88])\n",
      "backbone2.0.convs.1.weight conv2d_12/Kernel (96, 1, 1, 88) torch.Size([96, 88, 1, 1])\n",
      "backbone2.0.convs.1.bias conv2d_12/Bias (96,) torch.Size([96])\n",
      "backbone2.1.convs.0.weight depthwise_conv2d_12/Kernel (1, 3, 3, 96) torch.Size([96, 1, 3, 3])\n",
      "backbone2.1.convs.0.bias depthwise_conv2d_12/Bias (96,) torch.Size([96])\n",
      "backbone2.1.convs.1.weight conv2d_13/Kernel (96, 1, 1, 96) torch.Size([96, 96, 1, 1])\n",
      "backbone2.1.convs.1.bias conv2d_13/Bias (96,) torch.Size([96])\n",
      "backbone2.2.convs.0.weight depthwise_conv2d_13/Kernel (1, 3, 3, 96) torch.Size([96, 1, 3, 3])\n",
      "backbone2.2.convs.0.bias depthwise_conv2d_13/Bias (96,) torch.Size([96])\n",
      "backbone2.2.convs.1.weight conv2d_14/Kernel (96, 1, 1, 96) torch.Size([96, 96, 1, 1])\n",
      "backbone2.2.convs.1.bias conv2d_14/Bias (96,) torch.Size([96])\n",
      "backbone2.3.convs.0.weight depthwise_conv2d_14/Kernel (1, 3, 3, 96) torch.Size([96, 1, 3, 3])\n",
      "backbone2.3.convs.0.bias depthwise_conv2d_14/Bias (96,) torch.Size([96])\n",
      "backbone2.3.convs.1.weight conv2d_15/Kernel (96, 1, 1, 96) torch.Size([96, 96, 1, 1])\n",
      "backbone2.3.convs.1.bias conv2d_15/Bias (96,) torch.Size([96])\n",
      "backbone2.4.convs.0.weight depthwise_conv2d_15/Kernel (1, 3, 3, 96) torch.Size([96, 1, 3, 3])\n",
      "backbone2.4.convs.0.bias depthwise_conv2d_15/Bias (96,) torch.Size([96])\n",
      "backbone2.4.convs.1.weight conv2d_16/Kernel (96, 1, 1, 96) torch.Size([96, 96, 1, 1])\n",
      "backbone2.4.convs.1.bias conv2d_16/Bias (96,) torch.Size([96])\n",
      "classifier_8.weight classificator_8/Kernel (2, 1, 1, 88) torch.Size([2, 88, 1, 1])\n",
      "classifier_8.bias classificator_8/Bias (2,) torch.Size([2])\n",
      "classifier_16.weight classificator_16/Kernel (6, 1, 1, 96) torch.Size([6, 96, 1, 1])\n",
      "classifier_16.bias classificator_16/Bias (6,) torch.Size([6])\n",
      "regressor_8.weight regressor_8/Kernel (32, 1, 1, 88) torch.Size([32, 88, 1, 1])\n",
      "regressor_8.bias regressor_8/Bias (32,) torch.Size([32])\n",
      "regressor_16.weight regressor_16/Kernel (96, 1, 1, 96) torch.Size([96, 96, 1, 1])\n",
      "regressor_16.bias regressor_16/Bias (96,) torch.Size([96])\n"
     ]
    }
   ],
   "source": [
    "new_state_dict = OrderedDict()\n",
    "\n",
    "for dst, src in convert.items():\n",
    "    W = get_weights(src)\n",
    "    print(dst, src, W.shape, net.state_dict()[dst].shape)\n",
    "\n",
    "    if W.ndim == 4:\n",
    "        if W.shape[0] == 1:\n",
    "            W = W.transpose((3, 0, 1, 2))  # depthwise conv\n",
    "        else:\n",
    "            W = W.transpose((0, 3, 1, 2))  # regular conv\n",
    "    \n",
    "    new_state_dict[dst] = torch.from_numpy(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(new_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No errors? Then the conversion was successful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"blazeface.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
